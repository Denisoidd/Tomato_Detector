{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tomato_train.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yeuGelHL-GC"
      },
      "source": [
        "#Import project\n",
        "All the code is mostly written in `.py` files at git repository.\n",
        "\n",
        "Colab is used like a wrapper in order to train the network faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvChHNx2MalR"
      },
      "source": [
        "# if directory already exists it's better to remove it\n",
        "!rm -r Tomato_Detector"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dK_kH91G58A",
        "outputId": "fafc4db5-9d65-4cd4-dd29-a366940b49bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# clone existing repo\n",
        "!git clone https://github.com/Denisoidd/Tomato_Detector"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Tomato_Detector'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 96 (delta 39), reused 78 (delta 26), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV7Rm01tf_hD"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZNxESfalgh"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Tomato_Detector/assignment_imgs'):\n",
        "  os.makedirs('Tomato_Detector/assignment_imgs')\n",
        "\n",
        "if not os.path.exists('Tomato_Detector/annotations'):\n",
        "  os.makedirs('Tomato_Detector/annotations')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k31v3jSiRLZY"
      },
      "source": [
        "#Install required dependencies\n",
        "Here we need to explicitely install the latest version of tensorflow.\n",
        "\n",
        "All other packages are already installed in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqMvegJvROTQ",
        "outputId": "5814fa18-5bba-4c53-968d-604775b05783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we will install the latest version of tf\n",
        "!pip install tf-nightly"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/6d/71a1b3633ca6849503999a4ada2550bc0f1234c02170033c51553ba2a967/tf_nightly-2.4.0.dev20201007-cp36-cp36m-manylinux2010_x86_64.whl (392.0MB)\n",
            "\u001b[K     |████████████████████████████████| 392.0MB 42kB/s \n",
            "\u001b[?25hCollecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/53/b418f567dda3790f8b0a9948bff7c51f411fdd5d8b5e6d5167bdfd47a0b0/tb_nightly-2.4.0a20201007-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/facc0264b446611249f02c1fffb374ebeafd4959889a42a7988362866544/tf_estimator_nightly-2.4.0.dev2020100701-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: flatbuffers, tb-nightly, tf-estimator-nightly, tf-nightly\n",
            "Successfully installed flatbuffers-1.12 tb-nightly-2.4.0a20201007 tf-estimator-nightly-2.4.0.dev2020100701 tf-nightly-2.4.0.dev20201007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMtAqeGe3Yq"
      },
      "source": [
        "#Upload dataset\n",
        "\n",
        "Upload `img_annotations.json` and `label_mapping.csv` to `Tomato_Detector/annotations`. Also upload your dataset to `Tomato_Detector/` in zip format and unzip it with code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvSDMSqf2Ar"
      },
      "source": [
        "Unzip file to the correct directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2qm355KgZje"
      },
      "source": [
        "path_to_zip_file = 'Tomato_Detector/assignment_imgs.zip'\n",
        "directory_to_extract_to = 'Tomato_Detector/'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQdyVm6ToZYg"
      },
      "source": [
        "If you use google drive you should run the code below otherwise don't bother :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1zHdg_5jxwD",
        "outputId": "f9482d46-8bcb-407a-d8a1-7d7a4ac3b11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHWMDPiClwq6",
        "outputId": "eabcc0dd-736b-4661-df61-a4ffd4376951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('gdrive/My Drive/kaggle/tomato/img_annotations.json', 'Tomato_Detector/annotations')\n",
        "shutil.copy('gdrive/My Drive/kaggle/tomato/label_mapping.csv', 'Tomato_Detector/annotations')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tomato_Detector/annotations/label_mapping.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QHFo0jOonie"
      },
      "source": [
        "path_to_zip_file = 'gdrive/My Drive/kaggle/tomato/assignment_imgs.zip'\n",
        "directory_to_extract_to = 'Tomato_Detector/'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLaZH4ohL82r"
      },
      "source": [
        "#Train network\n",
        "Already trained model is stored in `saved_model` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooDTdLAMWaw",
        "outputId": "82081085-1aae-4226-8a2d-86144895fe0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# launch train process\n",
        "!python Tomato_Detector/main.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-07 08:13:35.263970: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 08:13:35.264020: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Config loaded correctly\n",
            "Total number of images: 3000\n",
            "We have 542 tomato images\n",
            "We have 2458 images without tomatoes\n",
            "Found 3000 files belonging to 2 classes.\n",
            "Using 2400 files for training.\n",
            "2020-10-07 08:13:52.583254: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2020-10-07 08:13:52.584559: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-10-07 08:13:53.737105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-07 08:13:53.745910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-10-07 08:13:53.746255: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 08:13:53.746527: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 08:13:53.861173: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-10-07 08:13:53.877880: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-10-07 08:13:54.154966: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-10-07 08:13:54.155276: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 08:13:54.155422: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 08:13:54.155445: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2020-10-07 08:13:54.155885: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-10-07 08:13:54.156009: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2020-10-07 08:13:54.156050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-10-07 08:13:54.156063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
            "Train dataset prepared successfully\n",
            "Found 3000 files belonging to 2 classes.\n",
            "Using 600 files for validation.\n",
            "Val dataset prepared successfully\n",
            "Epoch 1/15\n",
            "2020-10-07 08:13:55.622945: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)\n",
            "2020-10-07 08:13:55.626773: I tensorflow/core/platform/profile_utils/cpu_utils.cc:108] CPU Frequency: 2200000000 Hz\n",
            "2020-10-07 08:13:58.027585: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25165824 exceeds 10% of free system memory.\n",
            "2020-10-07 08:13:58.267380: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25165824 exceeds 10% of free system memory.\n",
            "2020-10-07 08:13:58.525723: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25165824 exceeds 10% of free system memory.\n",
            "2020-10-07 08:13:58.799147: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25165824 exceeds 10% of free system memory.\n",
            "2020-10-07 08:13:59.078861: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25165824 exceeds 10% of free system memory.\n",
            "2020-10-07 08:14:05.925582: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 32 of 1000\n",
            "2020-10-07 08:14:14.785150: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:230] Shuffle buffer filled.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2292: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "2020-10-07 08:14:18.897790: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1377: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
            "75/75 [==============================] - 405s 5s/step - loss: 0.4980 - accuracy: 0.7975 - precision_m: 0.0141 - recall_m: 0.0647 - val_loss: 0.6491 - val_accuracy: 0.7983 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/15\n",
            "75/75 [==============================] - 376s 5s/step - loss: 0.5020 - accuracy: 0.8184 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5005 - val_accuracy: 0.7983 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 3/15\n",
            "75/75 [==============================] - 374s 5s/step - loss: 0.4369 - accuracy: 0.8314 - precision_m: 0.0115 - recall_m: 0.0034 - val_loss: 0.4421 - val_accuracy: 0.8017 - val_precision_m: 0.2895 - val_recall_m: 0.0593\n",
            "Epoch 4/15\n",
            "75/75 [==============================] - 374s 5s/step - loss: 0.3995 - accuracy: 0.8351 - precision_m: 0.1408 - recall_m: 0.0408 - val_loss: 0.4659 - val_accuracy: 0.7983 - val_precision_m: 0.0789 - val_recall_m: 0.0175\n",
            "Epoch 5/15\n",
            "75/75 [==============================] - 378s 5s/step - loss: 0.4390 - accuracy: 0.8093 - precision_m: 0.0453 - recall_m: 0.0146 - val_loss: 0.4291 - val_accuracy: 0.7967 - val_precision_m: 0.2368 - val_recall_m: 0.0417\n",
            "Epoch 6/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.4164 - accuracy: 0.8173 - precision_m: 0.0771 - recall_m: 0.0163 - val_loss: 0.4151 - val_accuracy: 0.7917 - val_precision_m: 0.3904 - val_recall_m: 0.1729\n",
            "Epoch 7/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.4120 - accuracy: 0.7991 - precision_m: 0.2358 - recall_m: 0.0901 - val_loss: 0.4067 - val_accuracy: 0.7983 - val_precision_m: 0.4474 - val_recall_m: 0.2887\n",
            "Epoch 8/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3991 - accuracy: 0.8079 - precision_m: 0.3235 - recall_m: 0.1043 - val_loss: 0.3951 - val_accuracy: 0.8000 - val_precision_m: 0.4570 - val_recall_m: 0.2129\n",
            "Epoch 9/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3872 - accuracy: 0.8134 - precision_m: 0.3506 - recall_m: 0.1393 - val_loss: 0.4021 - val_accuracy: 0.8033 - val_precision_m: 0.4474 - val_recall_m: 0.1677\n",
            "Epoch 10/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3968 - accuracy: 0.8095 - precision_m: 0.3891 - recall_m: 0.1905 - val_loss: 0.4021 - val_accuracy: 0.8033 - val_precision_m: 0.4035 - val_recall_m: 0.1369\n",
            "Epoch 11/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3783 - accuracy: 0.8280 - precision_m: 0.4865 - recall_m: 0.1936 - val_loss: 0.4026 - val_accuracy: 0.8100 - val_precision_m: 0.5499 - val_recall_m: 0.3867\n",
            "Epoch 12/15\n",
            "75/75 [==============================] - 374s 5s/step - loss: 0.3653 - accuracy: 0.8379 - precision_m: 0.4831 - recall_m: 0.2317 - val_loss: 0.3821 - val_accuracy: 0.8133 - val_precision_m: 0.5763 - val_recall_m: 0.3646\n",
            "Epoch 13/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3576 - accuracy: 0.8316 - precision_m: 0.4913 - recall_m: 0.2638 - val_loss: 0.3740 - val_accuracy: 0.8217 - val_precision_m: 0.5939 - val_recall_m: 0.4255\n",
            "Epoch 14/15\n",
            "75/75 [==============================] - 375s 5s/step - loss: 0.3985 - accuracy: 0.8226 - precision_m: 0.5893 - recall_m: 0.2913 - val_loss: 0.3740 - val_accuracy: 0.8250 - val_precision_m: 0.6052 - val_recall_m: 0.4942\n",
            "Epoch 15/15\n",
            "75/75 [==============================] - 374s 5s/step - loss: 0.3689 - accuracy: 0.8283 - precision_m: 0.4985 - recall_m: 0.3111 - val_loss: 0.3896 - val_accuracy: 0.8400 - val_precision_m: 0.6736 - val_recall_m: 0.4541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8_cqduLgzS9"
      },
      "source": [
        "#Test\n",
        "\n",
        "Put the image into `test_data` directory and at the command below\n",
        "instead of <...> write its name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qka74X62g6L1",
        "outputId": "74c7ffd3-c3d8-475f-b347-3924d2a58f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python Tomato_Detector/test.py 1.jpg"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-07 19:57:30.511921: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 19:57:30.511963: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2020-10-07 19:57:32.033731: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2020-10-07 19:57:32.034741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-10-07 19:57:32.066939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-07 19:57:32.067495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-10-07 19:57:32.067624: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 19:57:32.067711: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 19:57:32.069447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-10-07 19:57:32.069825: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-10-07 19:57:32.071791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-10-07 19:57:32.071930: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 19:57:32.072032: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-10-07 19:57:32.072049: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2020-10-07 19:57:32.072281: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-10-07 19:57:32.072407: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2020-10-07 19:57:32.072441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-10-07 19:57:32.072452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
            "\n",
            "There are some tomatoes in the food\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAfy0lr4np-4"
      },
      "source": [
        "Once you've run the code you can take a look at `activation.png` to see combined activations of last convolution layer with dense weights (Discriminative localization)"
      ]
    }
  ]
}